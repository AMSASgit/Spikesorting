{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\amsas\\Desktop\\ADLab\\rishika_data\\Bayleef_2025-01-29_11-58-18_007\\Record Node 101\\experiment1\\recording1\\Analysis\")\n",
    "print(os.getcwd())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "import spikeinterface as si  # import core only\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.postprocessing as spost\n",
    "import spikeinterface.qualitymetrics as sqm\n",
    "import spikeinterface.comparison as sc\n",
    "import spikeinterface.exporters as sexp\n",
    "import spikeinterface.curation as scur\n",
    "import spikeinterface.widgets as sw\n",
    "import numpy as np\n",
    "from probeinterface import ProbeGroup, generate_tetrode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e4870",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=r\"C:\\Users\\amsas\\Desktop\\ADLab\\rishika_data\\Bayleef_2025-01-29_11-58-18_007\\Record Node 101\\experiment1\\recording1\"\n",
    "\n",
    "recording = se.read_openephys(folder_path=r\"C:\\Users\\amsas\\Desktop\\ADLab\\rishika_data\\Bayleef_2025-01-29_11-58-18_007\\Record Node 101\\experiment1\\recording1\", stream_id = '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e7077",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recording)\n",
    "print(\"Sampling frequency:\", recording.get_sampling_frequency())\n",
    "print(\"Num channels:\", len(recording.get_channel_ids()))\n",
    "print(\"Num segments:\", recording.get_num_segments())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0eef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 1000 timepoints from all channels in the first segment\n",
    "traces = recording.get_traces(segment_index=0, end_frame=1000)\n",
    "print(traces.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b7d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recording.get_channel_ids())\n",
    "\n",
    "selection = ['CH'+str(i) for i in np.arange(1,65)]\n",
    "ephys_recording = recording.select_channels(selection)\n",
    "\n",
    "print(ephys_recording.get_channel_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda908ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD CHANNEL MAP CODE\n",
    "# channelmapoe = [40, 38, 36, 34,\n",
    "#                 48, 46, 44, 42,\n",
    "#                 56, 54, 52, 50,\n",
    "#                 58, 64, 62, 60,\n",
    "#                 63, 61, 59, 57,\n",
    "#                 55, 53, 51, 49,\n",
    "#                 47, 45, 43, 41,\n",
    "#                 39, 37, 35, 33,\n",
    "#                 25, 27, 29, 31,\n",
    "#                 17, 19, 21, 23,\n",
    "#                 9, 11, 13, 15,\n",
    "#                 1, 3, 5, 7,\n",
    "#                 4, 6, 8, 2,\n",
    "#                 10, 12, 14, 16,\n",
    "#                 18, 20, 22, 24,\n",
    "#                 26, 28, 30, 32]\n",
    "\n",
    "# # subtract 1 to fit python indexing\n",
    "# channelmappythonic = np.array(channelmapoe) - 1\n",
    "\n",
    "# # create probegroup and set channel locations for sorting purposes\n",
    "# probegroup = ProbeGroup()\n",
    "# for i in range(16): # we have 16 tetrodes\n",
    "#     tetrode = generate_tetrode()\n",
    "#     tetrode.move([i * 300, 0]) # make lots of space between tetrodes\n",
    "#     probegroup.add_probe(tetrode)\n",
    "\n",
    "# probegroup.set_global_device_channel_indices(channelmappythonic) # tetrodes arranged based on channelmap\n",
    "# ephys_recording = ephys_recording.set_probegroup(probegroup, group_mode='by_probe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Channel map (convert to 0-based indexing)\n",
    "channelmapoe = [40, 38, 36, 34,\n",
    "                48, 46, 44, 42,\n",
    "                56, 54, 52, 50,\n",
    "                58, 64, 62, 60,\n",
    "                63, 61, 59, 57,\n",
    "                55, 53, 51, 49,\n",
    "                47, 45, 43, 41,\n",
    "                39, 37, 35, 33,\n",
    "                25, 27, 29, 31,\n",
    "                17, 19, 21, 23,\n",
    "                9, 11, 13, 15,\n",
    "                1, 3, 5, 7,\n",
    "                4, 6, 8, 2,\n",
    "                10, 12, 14, 16,\n",
    "                18, 20, 22, 24,\n",
    "                26, 28, 30, 32]\n",
    "channelmappythonic = np.array(channelmapoe) - 1\n",
    "\n",
    "# Layout parameters\n",
    "row_tetrode_counts = [2, 6, 6, 2]\n",
    "x_spacing = 421.64\n",
    "y_spacing = 365.15\n",
    "row_shift = 210.82\n",
    "max_row_width = (max(row_tetrode_counts) - 1) * x_spacing + row_shift * (len(row_tetrode_counts) - 1)\n",
    "\n",
    "# Compute mirrored positions\n",
    "original_positions = []\n",
    "for row_index, count in enumerate(row_tetrode_counts):\n",
    "    x_offset = row_shift * row_index\n",
    "    row_width = (count - 1) * x_spacing\n",
    "    if row_index == 3:\n",
    "        row_start_x = max_row_width - row_width\n",
    "    else:\n",
    "        row_start_x = x_offset\n",
    "    y = row_index * y_spacing\n",
    "    row_positions = [(row_start_x + i * x_spacing, y) for i in range(count)]\n",
    "    original_positions.extend(row_positions)\n",
    "\n",
    "x_positions = [x for x, _ in original_positions]\n",
    "x_min, x_max = min(x_positions), max(x_positions)\n",
    "tetrode_positions = [((x_max - (x - x_min)), y) for x, y in original_positions]\n",
    "\n",
    "# Create dropdown widgets\n",
    "tetrode_widgets = []\n",
    "dropdown_options = [(f\"Tetrode {i}\", i) for i in range(16)]\n",
    "\n",
    "for idx in range(16):\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=dropdown_options,\n",
    "        value=idx,\n",
    "        description=f\"Pos {idx}:\",\n",
    "        layout=widgets.Layout(width='200px')\n",
    "    )\n",
    "    tetrode_widgets.append(dropdown)\n",
    "\n",
    "# Show initial layout and widgets\n",
    "def plot_layout_with_labels():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, (x, y) in enumerate(tetrode_positions):\n",
    "        plt.scatter(x, y, s=150)\n",
    "        plt.text(x, y - 20, f\"Pos {i}\", ha='center', fontsize=8)\n",
    "    plt.title(\"Tetrode Probe Layout (Mirrored)\")\n",
    "    plt.xlabel(\"X (microns)\")\n",
    "    plt.ylabel(\"Y (microns)\")\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Display interactive UI\n",
    "plot_layout_with_labels()\n",
    "display(widgets.VBox(tetrode_widgets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60503fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probeinterface.plotting import plot_probegroup\n",
    "\n",
    "# Validate and build the ProbeGroup\n",
    "assigned_tetrodes = [w.value for w in tetrode_widgets]\n",
    "if len(set(assigned_tetrodes)) != 16:\n",
    "    print(\"Error: Each source tetrode (0–15) must be used exactly once.\")\n",
    "else:\n",
    "    probegroup = ProbeGroup()\n",
    "    for i, assigned_index in enumerate(assigned_tetrodes):\n",
    "        tetrode = generate_tetrode()\n",
    "        tetrode.move(tetrode_positions[i])\n",
    "        channel_indices = channelmappythonic[assigned_index * 4: (assigned_index + 1) * 4]\n",
    "        tetrode.set_device_channel_indices(channel_indices)\n",
    "        tetrode.annotations['assigned_tetrode'] = assigned_index\n",
    "        probegroup.add_probe(tetrode)\n",
    "\n",
    "    # Show updated layout and probe group\n",
    "    clear_output()\n",
    "    plot_layout_with_labels()\n",
    "    plot_probegroup(probegroup, same_axes=True)\n",
    "\n",
    "    print(\"ProbeGroup built successfully!\")\n",
    "    print(\"Use with: ephys_recording = ephys_recording.set_probegroup(probegroup, group_mode='by_probe')\")\n",
    "\n",
    "    print(\"\\nFinal Tetrode Assignments:\")\n",
    "    for i, probe in enumerate(probegroup.probes):\n",
    "        tid = probe.annotations.get(\"assigned_tetrode\", \"Unknown\")\n",
    "        chans = list(probe.device_channel_indices)\n",
    "        print(f\"Pos {i} → Tetrode {tid} → Channels: {chans}\")\n",
    "    \n",
    "    ephys_recording = ephys_recording.set_probegroup(probegroup, group_mode='by_probe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probeinterface.plotting import plot_probegroup\n",
    "\n",
    "plot_probegroup(probegroup, same_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444f7029",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_channel_ids, info = spre.detect_bad_channels(ephys_recording)\n",
    "\n",
    "ephys_recording = ephys_recording.remove_channels(bad_channel_ids)\n",
    "print(bad_channel_ids)\n",
    "pprint(info)\n",
    "split_recording_dict = ephys_recording.split_by(\"group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f69b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_recording_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a45187",
   "metadata": {},
   "outputs": [],
   "source": [
    "for grouper, chan_rec in split_recording_dict.items():\n",
    "    try:\n",
    "        print(f\"--- Starting group {grouper} ---\")\n",
    "\n",
    "        filtered_recording = spre.bandpass_filter(chan_rec, freq_min=300, freq_max=6000)\n",
    "        referenced_recording = spre.common_reference(filtered_recording, operator='median')\n",
    "\n",
    "        sorting = ss.run_sorter(\n",
    "            sorter_name='mountainsort5',\n",
    "            recording=referenced_recording,\n",
    "            folder=f'mountainsort_sorting/group_{grouper}',\n",
    "            remove_existing_folder=True,\n",
    "            filter=False,\n",
    "            verbose=True,\n",
    "            n_jobs=8\n",
    "        )\n",
    "\n",
    "        sorting.save(folder=f'mountainsort_output/group_{grouper}', overwrite=True)\n",
    "        print(f'Sorted tetrode {grouper}')\n",
    "\n",
    "        # Skip empty sortings\n",
    "        if len(sorting.unit_ids) == 0:\n",
    "            print(f\"No units found in group {grouper}. Skipping analyzer.\")\n",
    "            continue\n",
    "\n",
    "        analyzer = si.create_sorting_analyzer(\n",
    "            sorting=sorting,\n",
    "            recording=referenced_recording,\n",
    "            format=\"zarr\",\n",
    "            folder=f'analyzer_mountainsort/group_{grouper}',\n",
    "            overwrite=True\n",
    "            )\n",
    "        print('Created analyzer')\n",
    "\n",
    "        job_kwargs = dict(n_jobs=8, chunk_duration=\"1s\", progress_bar=True)\n",
    "        compute_dict = {\n",
    "            'random_spikes': {'method': 'uniform', 'max_spikes_per_unit': 500},\n",
    "            'waveforms': {'ms_before': 1.0, 'ms_after': 2.0},\n",
    "            'isi_histograms': {'window_ms': 50.0, 'bin_ms': 1.0, 'method': 'auto'},\n",
    "            'correlograms': {'window_ms': 50.0, 'bin_ms': 1.0, 'method': 'auto'},\n",
    "            'templates': {'operators': [\"average\", \"median\", \"std\"]},\n",
    "            'spike_amplitudes': {},\n",
    "            'unit_locations': {},\n",
    "            'template_similarity': {},\n",
    "            'noise_levels': {},\n",
    "            'quality_metrics': {}\n",
    "        }\n",
    "\n",
    "        analyzer.compute(compute_dict, **job_kwargs)\n",
    "        print(f'Computed features for group {grouper}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in group {grouper}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f6630",
   "metadata": {},
   "outputs": [],
   "source": [
    "for grouper, chan_rec in split_recording_dict.items():\n",
    "    try:\n",
    "        print(f\"--- Starting group {grouper} ---\")\n",
    "\n",
    "        analyzer_path = rf\"C:\\Users\\amsas\\Desktop\\ADLab\\rishika_data\\Bayleef_2025-01-29_11-58-18_007\\Record Node 101\\experiment1\\recording1\\Analysis\\analyzer_mountainsort\\group_{group_id}.zarr\"\n",
    "\n",
    "        sorting_analyzer= si.load_sorting_analyzer(analyzer_path)\n",
    "\n",
    "        analyzer=sorting_analyzer\n",
    "\n",
    "        job_kwargs = dict(n_jobs=8, chunk_duration=\"1s\", progress_bar=True)\n",
    "        compute_dict = {\n",
    "            'quality_metrics': {}\n",
    "        }\n",
    "\n",
    "        analyzer.compute(compute_dict, **job_kwargs)\n",
    "        print(f'Computed features for group {grouper}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in group {grouper}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f59f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_id, chan_rec in split_recording_dict.items():\n",
    "    print(f\"\\nGroup {group_id}\")\n",
    "    try:\n",
    "        traces = chan_rec.get_traces()\n",
    "        channel_ids = chan_rec.get_channel_ids()\n",
    "        means = np.mean(traces, axis=0)\n",
    "        stds = np.std(traces, axis=0)\n",
    "\n",
    "        for ch_id, mean, std in zip(channel_ids, means, stds):\n",
    "            print(f\"  Channel {ch_id}: mean = {mean:.2f}, std = {std:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading traces from group {group_id}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8fe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_id in split_recording_dict.keys():\n",
    "    print(f\"Group {group_id}: {split_recording_dict[group_id].get_channel_ids()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca6df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Create an output folder if it doesn't exist\n",
    "# output_dir = \"channel_plots\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# for group_id, rec in split_recording_dict.items():\n",
    "#     print(f\"Processing group {group_id}...\")\n",
    "#     try:\n",
    "#         traces = rec.get_traces(start_frame=0, end_frame=15000)  # shape: (timepoints, channels)\n",
    "#         channel_ids = rec.get_channel_ids()\n",
    "\n",
    "#         for i, ch_id in enumerate(channel_ids):\n",
    "#             trace = traces[:, i]\n",
    "#             plt.figure(figsize=(15, 4))\n",
    "#             plt.plot(trace, linewidth=0.5)\n",
    "#             plt.title(f\"Group {group_id} - Channel {ch_id}\")\n",
    "#             plt.xlabel(\"Time (samples)\")\n",
    "#             plt.ylabel(\"Amplitude\")\n",
    "#             plt.tight_layout()\n",
    "\n",
    "#             filename = f\"group{group_id}_channel{ch_id}.png\"\n",
    "#             filepath = os.path.join(output_dir, filename)\n",
    "#             plt.savefig(filepath, dpi=150)\n",
    "#             plt.close()\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in group {group_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466dc603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #spikeinterface curation gui\n",
    "\n",
    "# sorting_analyzer = si.load_sorting_analyzer(r\"C:\\Users\\amsas\\Desktop\\ADLab\\rishika_data\\Bayleef_2025-01-29_11-58-18_007\\Record Node 101\\experiment1\\recording1\\Analysis\\analyzer_mountainsort\\group_9.zarr\")\n",
    "# sw.plot_sorting_summary(sorting_analyzer, backend=\"spikeinterface_gui\", curation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af9eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_id in range(16):\n",
    "    try:\n",
    "        analyzer_path = rf\"C:\\Users\\amsas\\Desktop\\ADLab\\rishika_data\\Bayleef_2025-01-29_11-58-18_007\\Record Node 101\\experiment1\\recording1\\Analysis\\analyzer_mountainsort\\group_{group_id}.zarr\"\n",
    "\n",
    "        sorting_analyzer= si.load_sorting_analyzer(analyzer_path)\n",
    "\n",
    "        print(sorting_analyzer.get_loaded_extension_names())\n",
    "\n",
    "        metrics = sqm.compute_quality_metrics(sorting_analyzer, metric_names=[\n",
    "            \"snr\",\n",
    "            \"isi_violation\",\n",
    "            \"nearest_neighbor\",\n",
    "            \"firing_rate\",\n",
    "            \"isi_violation\",\n",
    "            \"l_ratio\",\n",
    "            \"isolation_distance\",\n",
    "            \"presence_ratio\"\n",
    "            ])\n",
    "        \n",
    "        print(f\"Metrics for group_{group_id}\")\n",
    "        print(metrics)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in group {group_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.exporters import export_to_phy\n",
    "\n",
    "for group_id in range(16):\n",
    "    try:\n",
    "        analyzer_path = rf\"C:\\Users\\amsas\\Desktop\\ADLab\\rishika_data\\Bayleef_2025-01-29_11-58-18_007\\Record Node 101\\experiment1\\recording1\\Analysis\\analyzer_mountainsort\\group_{group_id}.zarr\"\n",
    "        output_path = rf\"C:\\Users\\amsas\\Desktop\\ADLab\\rishika_data\\Bayleef_2025-01-29_11-58-18_007\\Record Node 101\\experiment1\\recording1\\Analysis\\phy_output\\group_{group_id}_phy\"\n",
    "\n",
    "        if not os.path.exists(analyzer_path):\n",
    "            print(f\"Skipping group {group_id} — no analyzer found.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Exporting group {group_id} to Phy...\")\n",
    "\n",
    "        sorting_analyzer = si.load_sorting_analyzer(analyzer_path)\n",
    "\n",
    "        export_to_phy(\n",
    "            sorting_analyzer=sorting_analyzer,\n",
    "            output_folder=output_path,\n",
    "            compute_pc_features=True,\n",
    "            compute_amplitudes=True,\n",
    "        )\n",
    "\n",
    "        print(f\"Exported to Phy for group {group_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting group {group_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18893618",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keep_units = {}\n",
    "\n",
    "for group_id in range(16):\n",
    "    try:\n",
    "        analyzer_path = rf\"C:\\Users\\amsas\\Desktop\\ADLab\\rishika_data\\Bayleef_2025-01-29_11-58-18_007\\Record Node 101\\experiment1\\recording1\\Analysis\\analyzer_mountainsort\\group_{group_id}.zarr\"\n",
    "        if not os.path.exists(analyzer_path):\n",
    "            print(f\"Skipping group {group_id} — analyzer not found.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nEvaluating group {group_id}...\")\n",
    "\n",
    "        sorting_analyzer = si.load_sorting_analyzer(analyzer_path)\n",
    "\n",
    "        # Compute noise_levels if missing\n",
    "        if not sorting_analyzer.has_extension(\"noise_levels\"):\n",
    "            print(\"Computing noise_levels...\")\n",
    "            sorting_analyzer.compute(\"noise_levels\")\n",
    "\n",
    "        # Then compute quality metrics\n",
    "        if not sorting_analyzer.has_extension(\"quality_metrics\"):\n",
    "            print(\"Computing quality_metrics...\")\n",
    "            sorting_analyzer.compute(\"quality_metrics\")\n",
    "\n",
    "        # Retrieve metrics\n",
    "        metrics = sorting_analyzer.get_extension(\"quality_metrics\").get_data()\n",
    "\n",
    "        # Apply your thresholds\n",
    "        keep_mask = (\n",
    "            (metrics[\"snr\"] > 7.5) &\n",
    "            (metrics[\"isi_violations_ratio\"] < 0.2) &\n",
    "            (metrics[\"nn_hit_rate\"] > 0.90)\n",
    "        )\n",
    "\n",
    "        keep_unit_ids = keep_mask[keep_mask].index.to_list()\n",
    "        total_units = len(metrics)\n",
    "        good_units = len(keep_unit_ids)\n",
    "\n",
    "        print(f\"Total units in group {group_id}: {total_units}\")\n",
    "        print(f\"Good units in group {group_id}: {good_units} -> {keep_unit_ids}\")\n",
    "\n",
    "        all_keep_units[group_id] = keep_unit_ids\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing group {group_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f3fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_id in range(16):\n",
    "    try:\n",
    "        analyzer_path = rf\"C:\\Users\\amsas\\Desktop\\ADLab\\rishika_data\\Bayleef_2025-01-29_11-58-18_007\\Record Node 101\\experiment1\\recording1\\Analysis\\analyzer_mountainsort\\group_{group_id}.zarr\"\n",
    "        if not os.path.exists(analyzer_path):\n",
    "            print(f\"Skipping group {group_id} — analyzer not found.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Plotting group {group_id}...\")\n",
    "        sorting_analyzer = si.load_sorting_analyzer(analyzer_path)\n",
    "        unit_ids = sorting_analyzer.sorting.unit_ids\n",
    "\n",
    "        if len(unit_ids) == 0:\n",
    "            print(f\"Group {group_id} has no units. Skipping plots.\")\n",
    "            continue\n",
    "\n",
    "        # Labeling each figure with the group ID\n",
    "        def label_figure(title):\n",
    "            plt.suptitle(f\"Group {group_id} - {title}\", fontsize=16)\n",
    "\n",
    "        sw.plot_unit_waveforms(sorting_analyzer, unit_ids=unit_ids, figsize=(16, 4))\n",
    "        label_figure(\"Unit Waveforms\")\n",
    "\n",
    "        sw.plot_unit_templates(sorting_analyzer, unit_ids=unit_ids, ncols=5, figsize=(16,8))\n",
    "        label_figure(\"Unit Templates\")\n",
    "\n",
    "        sw.plot_unit_locations(sorting_analyzer, unit_ids=unit_ids)\n",
    "        label_figure(\"Unit Locations\")\n",
    "\n",
    "        sw.plot_amplitudes(sorting_analyzer, plot_histograms=True, figsize=(12,8))\n",
    "        label_figure(\"Amplitudes\")\n",
    "\n",
    "        plt.show()  # Show all plots before next group\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in group {group_id}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si_env_rolling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
